{"cells":[{"source":"![cyber_photo](cyber_photo.jpg)\n\nCyber threats are a growing concern for organizations worldwide. These threats take many forms, including malware, phishing, and denial-of-service (DOS) attacks, compromising sensitive information and disrupting operations. The increasing sophistication and frequency of these attacks make it imperative for organizations to adopt advanced security measures. Traditional threat detection methods often fall short due to their inability to adapt to new and evolving threats. This is where deep learning models come into play.\n\nDeep learning models can analyze vast amounts of data and identify patterns that may not be immediately obvious to human analysts. By leveraging these models, organizations can proactively detect and mitigate cyber threats, safeguarding their sensitive information and ensuring operational continuity.\n\nAs a cybersecurity analyst, you identify and mitigate these threats. In this project, you will design and implement a deep learning model to detect cyber threats. The BETH dataset simulates real-world logs, providing a rich source of information for training and testing your model. The data has already undergone preprocessing, and we have a target label, `sus_label`, indicating whether an event is malicious (1) or benign (0).\n\nBy successfully developing this model, you will contribute to enhancing cybersecurity measures and protecting organizations from potentially devastating cyber attacks.","metadata":{},"id":"51be1f3d-e425-4d6d-9c05-fb6d98664c68","cell_type":"markdown"},{"source":"\n### The Data\n\n| Column     | Description              |\n|------------|--------------------------|\n|`processId`|The unique identifier for the process that generated the event - int64 |\n|`threadId`|ID for the thread spawning the log - int64|\n|`parentProcessId`|Label for the process spawning this log - int64|\n|`userId`|ID of user spawning the log|Numerical - int64|\n|`mountNamespace`|Mounting restrictions the process log works within - int64|\n|`argsNum`|Number of arguments passed to the event - int64|\n|`returnValue`|Value returned from the event log (usually 0) - int64|\n|`sus_label`|Binary label as suspicous event (1 is suspicious, 0 is not) - int64|\n\nMore information on the dataset: [BETH dataset](accreditation.md)","metadata":{},"id":"8811256f-f887-4867-903e-837238fbb648","cell_type":"markdown"},{"source":"# Import required libraries\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as functional\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.optim as optim\nfrom torchmetrics import Accuracy\nfrom sklearn.metrics import accuracy_score  # uncomment to use sklearn","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1748531282384,"lastExecutedByKernel":"6033f240-3f94-465d-b726-f1022fdc301d","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import required libraries\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as functional\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.optim as optim\nfrom torchmetrics import Accuracy\nfrom sklearn.metrics import accuracy_score  # uncomment to use sklearn"},"id":"75892dec-9424-4c92-bf8e-2f9847b7d7cd","cell_type":"code","execution_count":73,"outputs":[]},{"source":"# Load preprocessed data\ntrain_df = pd.read_csv('labelled_train.csv')\ntest_df = pd.read_csv('labelled_test.csv')\nval_df = pd.read_csv('labelled_validation.csv')\n\n# View the first 5 rows of training set\ntrain_df.head()","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":550,"type":"dataFrame","tableState":{"customFilter":{"const":{"type":"boolean","valid":true,"value":true},"id":"1e5abd9f-9a6c-4216-a360-eeb7015ac529","nodeType":"const"}}}}},"id":"e52e231f-71b5-4dfc-81f9-a3321ff78047","cell_type":"code","execution_count":74,"outputs":[{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"processId","type":"integer"},{"name":"threadId","type":"integer"},{"name":"parentProcessId","type":"integer"},{"name":"userId","type":"integer"},{"name":"mountNamespace","type":"integer"},{"name":"argsNum","type":"integer"},{"name":"returnValue","type":"integer"},{"name":"sus_label","type":"integer"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":{"index":[0,1,2,3,4],"processId":[381,381,381,7347,7347],"threadId":[7337,7337,7337,7347,7347],"parentProcessId":[1,1,1,7341,7341],"userId":[100,100,100,0,0],"mountNamespace":[4026532231,4026532231,4026532231,4026531840,4026531840],"argsNum":[5,1,0,2,4],"returnValue":[0,0,0,-2,0],"sus_label":[1,1,1,1,1]}},"total_rows":5,"truncation_type":null},"text/plain":"   processId  threadId  parentProcessId  ...  argsNum  returnValue  sus_label\n0        381      7337                1  ...        5            0          1\n1        381      7337                1  ...        1            0          1\n2        381      7337                1  ...        0            0          1\n3       7347      7347             7341  ...        2           -2          1\n4       7347      7347             7341  ...        4            0          1\n\n[5 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>processId</th>\n      <th>threadId</th>\n      <th>parentProcessId</th>\n      <th>userId</th>\n      <th>mountNamespace</th>\n      <th>argsNum</th>\n      <th>returnValue</th>\n      <th>sus_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>381</td>\n      <td>7337</td>\n      <td>1</td>\n      <td>100</td>\n      <td>4026532231</td>\n      <td>5</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>381</td>\n      <td>7337</td>\n      <td>1</td>\n      <td>100</td>\n      <td>4026532231</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>381</td>\n      <td>7337</td>\n      <td>1</td>\n      <td>100</td>\n      <td>4026532231</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7347</td>\n      <td>7347</td>\n      <td>7341</td>\n      <td>0</td>\n      <td>4026531840</td>\n      <td>2</td>\n      <td>-2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7347</td>\n      <td>7347</td>\n      <td>7341</td>\n      <td>0</td>\n      <td>4026531840</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{"application/com.datacamp.data-table.v2+json":{"status":"success"}},"execution_count":74}]},{"source":"# Loading and Scaling Data\n#Separate features and labels to scale the features and convert data to PyTorch tensors\nX_train = train_df.drop('sus_label', axis=1).values\ny_train = train_df['sus_label'].values\nX_test = test_df.drop('sus_label', axis=1).values\ny_test = test_df['sus_label'].values\nX_val = val_df.drop('sus_label', axis=1).values\ny_val = val_df['sus_label'].values\n\n# Separating input features (X) from target labels (y) for supervised learning.\n","metadata":{"executionCancelledAt":null,"executionTime":67,"lastExecutedAt":1748531282808,"lastExecutedByKernel":"6033f240-3f94-465d-b726-f1022fdc301d","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Loading and Scaling Data\n#Separate features and labels to scale the features and convert data to PyTorch tensors\nX_train = train_df.drop('sus_label', axis=1).values\ny_train = train_df['sus_label'].values\nX_test = test_df.drop('sus_label', axis=1).values\ny_test = test_df['sus_label'].values\nX_val = val_df.drop('sus_label', axis=1).values\ny_val = val_df['sus_label'].values\n\n# Separating input features (X) from target labels (y) for supervised learning.\n"},"id":"6b7ee389-d5ef-4c4d-bc7a-4d0be0e1c6e1","cell_type":"code","execution_count":75,"outputs":[]},{"source":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nX_val = scaler.transform(X_val)\n\n'''\nFeature Scaling - Standardizes features to have mean=0 and std=1. This is crucial because:\n\nNeural networks are sensitive to input scale\nPrevents features with larger values from dominating\nHelps with gradient descent convergence\n'''","metadata":{"executionCancelledAt":null,"executionTime":64,"lastExecutedAt":1748531282873,"lastExecutedByKernel":"6033f240-3f94-465d-b726-f1022fdc301d","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nX_val = scaler.transform(X_val)\n\n'''\nFeature Scaling - Standardizes features to have mean=0 and std=1. This is crucial because:\n\nNeural networks are sensitive to input scale\nPrevents features with larger values from dominating\nHelps with gradient descent convergence\n'''"},"cell_type":"code","id":"9d1cb1f8-7e6d-46f9-8d2a-3b4f2c044cdd","outputs":[{"output_type":"execute_result","data":{"text/plain":"'\\nFeature Scaling - Standardizes features to have mean=0 and std=1. This is crucial because:\\n\\nNeural networks are sensitive to input scale\\nPrevents features with larger values from dominating\\nHelps with gradient descent convergence\\n'"},"metadata":{},"execution_count":76}],"execution_count":76},{"source":"X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\nX_val_tensor = torch.tensor(X_val, dtype=torch.float32)\ny_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n\n\n# Converting NumPy arrays to PyTorch tensors, which are the fundamental data structure for PyTorch operations and GPU acceleration.","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1748531282924,"lastExecutedByKernel":"6033f240-3f94-465d-b726-f1022fdc301d","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\nX_val_tensor = torch.tensor(X_val, dtype=torch.float32)\ny_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n\n\n# Converting NumPy arrays to PyTorch tensors, which are the fundamental data structure for PyTorch operations and GPU acceleration."},"cell_type":"code","id":"279424bf-fbb4-4fd4-86de-0abf5d196f32","outputs":[],"execution_count":77},{"source":"model = nn.Sequential(\n    nn.Linear(X_train.shape[1], 128),  # Input layer → 128 neurons\n    nn.ReLU(),                         # Activation function\n    nn.Linear(128, 64),                # Hidden layer → 64 neurons\n    nn.ReLU(),                         # Activation function\n    nn.Linear(64, 1),                  # Output layer → 1 neuron\n    nn.Sigmoid()                       # Final activation for binary classification\n)\n\n'''\nFeedforward Neural Network: Data flows forward through layers\nHidden Layers: Two hidden layers (128 and 64 neurons) for learning complex patterns\nReLU Activation: Rectified Linear Unit (max(0,x)) - prevents vanishing gradients\nSigmoid Output: Outputs probability between 0-1 for binary classification\n'''","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1748531282972,"lastExecutedByKernel":"6033f240-3f94-465d-b726-f1022fdc301d","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"model = nn.Sequential(\n    nn.Linear(X_train.shape[1], 128),  # Input layer → 128 neurons\n    nn.ReLU(),                         # Activation function\n    nn.Linear(128, 64),                # Hidden layer → 64 neurons\n    nn.ReLU(),                         # Activation function\n    nn.Linear(64, 1),                  # Output layer → 1 neuron\n    nn.Sigmoid()                       # Final activation for binary classification\n)\n\n'''\nFeedforward Neural Network: Data flows forward through layers\nHidden Layers: Two hidden layers (128 and 64 neurons) for learning complex patterns\nReLU Activation: Rectified Linear Unit (max(0,x)) - prevents vanishing gradients\nSigmoid Output: Outputs probability between 0-1 for binary classification\n'''"},"cell_type":"code","id":"36f9830e-7fd9-495a-a227-43346a3a93f9","outputs":[{"output_type":"execute_result","data":{"text/plain":"'\\nFeedforward Neural Network: Data flows forward through layers\\nHidden Layers: Two hidden layers (128 and 64 neurons) for learning complex patterns\\nReLU Activation: Rectified Linear Unit (max(0,x)) - prevents vanishing gradients\\nSigmoid Output: Outputs probability between 0-1 for binary classification\\n'"},"metadata":{},"execution_count":78}],"execution_count":78},{"source":"criterion = nn.BCELoss()  # ⚠️ Issue here - see below\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n                       \n'''\nLoss Function: Measures prediction error\nOptimizer: Updates model weights to minimize loss\nWeight Decay: L2 regularization to prevent overfitting\nLearning Rate: Controls step size in gradient descent\n'''","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1748531283024,"lastExecutedByKernel":"6033f240-3f94-465d-b726-f1022fdc301d","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"criterion = nn.BCELoss()  # ⚠️ Issue here - see below\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n                       \n'''\nLoss Function: Measures prediction error\nOptimizer: Updates model weights to minimize loss\nWeight Decay: L2 regularization to prevent overfitting\nLearning Rate: Controls step size in gradient descent\n'''"},"cell_type":"code","id":"e1ee90eb-564e-4677-ba6a-b564d014d6a3","outputs":[{"output_type":"execute_result","data":{"text/plain":"'\\nLoss Function: Measures prediction error\\nOptimizer: Updates model weights to minimize loss\\nWeight Decay: L2 regularization to prevent overfitting\\nLearning Rate: Controls step size in gradient descent\\n'"},"metadata":{},"execution_count":79}],"execution_count":79},{"source":"num_epoch = 10\nfor epoch in range(num_epoch):\n    model.train()           # Set to training mode\n    optimizer.zero_grad()   # Clear previous gradients\n    outputs = model(X_train_tensor)  # Forward pass\n    loss = criterion(outputs, y_train_tensor)  # Calculate loss\n    loss.backward()         # Backward pass (compute gradients)\n    optimizer.step()        # Update weights\n\n'''\nEpoch: One complete pass through the training data\nForward Pass: Input → Output prediction\nBackward Pass: Calculate gradients using backpropagation\nGradient Descent: Update weights to minimize loss\n'''","metadata":{"executionCancelledAt":null,"executionTime":49794,"lastExecutedAt":1748531332818,"lastExecutedByKernel":"6033f240-3f94-465d-b726-f1022fdc301d","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"num_epoch = 10\nfor epoch in range(num_epoch):\n    model.train()           # Set to training mode\n    optimizer.zero_grad()   # Clear previous gradients\n    outputs = model(X_train_tensor)  # Forward pass\n    loss = criterion(outputs, y_train_tensor)  # Calculate loss\n    loss.backward()         # Backward pass (compute gradients)\n    optimizer.step()        # Update weights\n\n'''\nEpoch: One complete pass through the training data\nForward Pass: Input → Output prediction\nBackward Pass: Calculate gradients using backpropagation\nGradient Descent: Update weights to minimize loss\n'''"},"cell_type":"code","id":"6e2e3148-a8cd-4112-9012-6b63d7a6f249","outputs":[{"output_type":"execute_result","data":{"text/plain":"'\\nEpoch: One complete pass through the training data\\nForward Pass: Input → Output prediction\\nBackward Pass: Calculate gradients using backpropagation\\nGradient Descent: Update weights to minimize loss\\n'"},"metadata":{},"execution_count":80}],"execution_count":80},{"source":"model.eval()  # Set to evaluation mode\nwith torch.no_grad():  # Disable gradient computation\n    y_predict_train = model(X_train_tensor).round()\n    y_predict_test = model(X_test_tensor).round()\n    y_predict_val = model(X_val_tensor).round()\n\naccuracy = Accuracy(task=\"binary\")\n\ntrain_accuracy = accuracy(y_predict_train, y_train_tensor)\ntest_accuracy = accuracy(y_predict_test, y_test_tensor)\nval_accuracy = accuracy(y_predict_val, y_val_tensor)\n\n# convert to int or float\ntrain_accuracy = train_accuracy.item()\ntest_accuracy = test_accuracy.item()\nval_accuracy = val_accuracy.item()\n\nprint(\"Training accuracy: {0}\".format(train_accuracy))\nprint(\"Validation accuracy: {0}\".format(val_accuracy))\nprint(\"Testing accuracy: {0}\".format(test_accuracy))","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":80,"type":"stream"}}},"cell_type":"code","id":"249be1f8-6918-47e9-a0e9-113695f10681","outputs":[{"output_type":"stream","name":"stdout","text":"Training accuracy: 0.9983371496200562\nValidation accuracy: 0.9958405494689941\nTesting accuracy: 0.09265109896659851\n"}],"execution_count":81},{"source":"Key Issues and Improvements\nThe comments in the code suggest several improvements:\n\nUse BCELoss instead of CrossEntropyLoss for binary classification\nUse Adam optimizer instead of SGD for potentially better convergence\nAdd validation during training to monitor overfitting\nUse DataLoader for batch processing (currently processes entire dataset at once)\n\nSummary of ML Concepts Implemented\n\nSupervised Learning: Learning from labeled examples\nBinary Classification: Predicting one of two classes\nDeep Learning: Multi-layer neural network\nFeature Scaling: Standardizing input features\nRegularization: Weight decay to prevent overfitting\nTrain-Validation-Test: Proper dataset splitting\nBackpropagation: Gradient-based learning algorithm","metadata":{},"cell_type":"markdown","id":"2e2906d6-dd26-4a95-be26-65945e46d470"}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}